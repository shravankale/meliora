__global__ void orcu_kernel88975(const int n, double* X, double* A, double* B) {
  const int tid=blockIdx.x*blockDim.x+threadIdx.x;
  const int gsize=gridDim.x*blockDim.x;
  int i2;
  for (int i1=tid; i1<=n-1; i1+=gsize) {
    for (i2=1; i2<=n-1; i2++ ) {
      X[i1*n+i2]=X[i1*n+i2]-X[i1*n+(i2-1)]*A[i1*n+i2]/B[i1*n+(i2-1)];
      B[i1*n+i2]=B[i1*n+i2]-A[i1*n+i2]*A[i1*n+i2]/B[i1*n+(i2-1)];
    }
  }
}
__global__ void orcu_kernel89595(const int n, int orcu_var89556, double* X, double* A, double* B) {
  const int tid=blockIdx.x*blockDim.x+threadIdx.x+orcu_var89556;
  const int gsize=gridDim.x*blockDim.x;
  int i2;
  for (int i1=tid; i1<=n-1; i1+=gsize) {
    for (i2=0; i2<=n-1; i2++ ) {
      X[i1*n+i2]=X[i1*n+i2]-X[(i1-1)*n+i2]*A[i1*n+i2]/B[(i1-1)*n+i2];
      B[i1*n+i2]=B[i1*n+i2]-A[i1*n+i2]*A[i1*n+i2]/B[(i1-1)*n+i2];
    }
  }
}
void adi(double* X, double* A, double* B) {
/*@ begin PerfTuning (
    def performance_params {
        param thread_count[]  = [32, 64];
        param block_count[]  = range(14,28,14);
        param inner_loop_unroll_fact[] = range(1, 5);
        param preferred_L1_cache[]  = [16, 48];
        param stream_count[] = [1, 2, 4, 8];
        param CFLAGS[] = ['-O3'];
    }
    def build {
      arg build_command = 'nvcc -arch=sm_75 @CFLAGS';
    }
  def performance_counter
  {
  arg repetitions = 5;
  }

  def search
  {
  arg algorithm = 'Randomlocal';
  arg total_runs = 1000;
  }

  def input_params
  {
  param T[] = [256];
  param N[] = [512]; 
  }
  
  def input_vars {
  decl static double X[N * (N+20)] = random;
  decl static double A[N * (N+20)] = random;
  decl static double B[N * (N+20)] = random;
  }
) @*/
/**-- (Generated by Orio) 
Best performance cost: 
  [0.128128, 0.118528, 0.127104, 0.126976, 0.126976] 
Tuned for specific problem sizes: 
  N = 512 
  T = 256 
Best performance parameters: 
  CFLAGS = -O3 
  block_count = 14 
  inner_loop_unroll_fact = 1 
  preferred_L1_cache = 48 
  stream_count = 8 
  thread_count = 32 
--**/
   


#define max(x,y)    ((x) > (y)? (x) : (y))
#define min(x,y)    ((x) < (y)? (x) : (y))


int t, n = N;


/*@ begin Loop (
 

for (t=0; t<=T-1; t++)
  {
  
  transform CUDA(threadCount=thread_count,
                 blockCount=block_count,
                 preferL1Size=preferred_L1_cache,
                 unrollInner=inner_loop_unroll_fact,
                 streamCount=stream_count)
  for (i1=0; i1<=n-1; i1++)
    for (i2=1; i2<=n-1; i2++)
    {
     X[i1 * n + i2] = X[i1 * n + i2] - X[i1 * n + (i2-1)] * A[i1 * n + i2] / B[i1 * n + (i2-1)];
     B[i1 * n + i2] = B[i1 * n + i2] - A[i1 * n + i2] * A[i1 * n + i2] / B[i1 * n + (i2-1)];
     }

  transform CUDA(threadCount=thread_count,
                 blockCount=block_count,
                 preferL1Size=preferred_L1_cache,
                 unrollInner=inner_loop_unroll_fact,
                 streamCount=stream_count)
   for (i1=1; i1<=n-1; i1++)
      for (i2=0; i2<=n-1; i2++)
      {
      X[i1 * n + i2] = X[i1 * n + i2] - X[(i1-1)  * n + i2] * A[i1 * n + i2] / B[(i1-1) * n + i2];
      B[i1 * n + i2] = B[i1 * n + i2] - A[i1 * n + i2] * A[i1 * n + i2] / B[(i1-1) * n + i2];
       }
  }


) @*/
for (t=0; t<=T-1; t++ ) {
  {
    cudaDeviceSynchronize();
    /*declare variables*/
    double* dev_X;
    double* dev_A;
    double* dev_B;
    int nthreads=32;
    int nstreams=8;
    /*calculate device dimensions with the cuda calculator*/
    dim3 dimGrid, dimBlock;
    dimBlock.x=nthreads;
    dimGrid.x=14;
    /*create streams*/
    int istream, soffset;
    cudaStream_t stream[nstreams+1];
    for (istream=0; istream<=nstreams; istream++ ) 
      cudaStreamCreate(&stream[istream]);
    int chunklen=n/nstreams;
    int chunkrem=n%nstreams;
    /*allocate device memory*/
    cudaMalloc(&dev_A,N *(N +20 )*sizeof(double));
    cudaHostRegister(A,N *(N +20 )*sizeof(double),cudaHostRegisterPortable);
    cudaMalloc(&dev_X,N *(N +20 )*sizeof(double));
    cudaHostRegister(X,N *(N +20 )*sizeof(double),cudaHostRegisterPortable);
    cudaMalloc(&dev_B,N *(N +20 )*sizeof(double));
    cudaHostRegister(B,N *(N +20 )*sizeof(double),cudaHostRegisterPortable);
    cudaDeviceSetCacheConfig(cudaFuncCachePreferL1);
    /*copy data from host to device*/
    cudaEventRecord(tstart,0);
    for (istream=0; istream<nstreams; istream++ ) {
      soffset=istream*chunklen;
      cudaMemcpyAsync(dev_A+soffset,A+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
      cudaMemcpyAsync(dev_X+soffset,X+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
      cudaMemcpyAsync(dev_B+soffset,B+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
    }
    if (chunkrem!=0) {
      soffset=istream*chunklen;
      cudaMemcpyAsync(dev_A+soffset,A+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
      cudaMemcpyAsync(dev_X+soffset,X+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
      cudaMemcpyAsync(dev_B+soffset,B+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
    }
    cudaEventRecord(tstop,0);
    cudaEventSynchronize(tstop);
    cudaEventElapsedTime(&orcu_transfer,tstart,tstop);
    cudaEventRecord(start,0);
    /*invoke device kernel*/
    int blks4chunk=dimGrid.x/nstreams;
    if (dimGrid.x%nstreams!=0) 
      blks4chunk++ ;
    for (istream=0; istream<nstreams; istream++ ) {
      soffset=istream*chunklen;
      orcu_kernel88975<<<blks4chunk,dimBlock,0,stream[istream]>>>(chunklen,dev_X+soffset,dev_A+soffset,dev_B+soffset);
    }
    if (chunkrem!=0) {
      soffset=istream*chunklen;
      orcu_kernel88975<<<blks4chunk,dimBlock,0,stream[istream]>>>(chunkrem,dev_X+soffset,dev_A+soffset,dev_B+soffset);
    }
    cudaEventRecord(stop,0);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&orcu_elapsed,start,stop);
    /*copy data from device to host*/
    for (istream=0; istream<nstreams; istream++ ) {
      soffset=istream*chunklen;
      cudaMemcpyAsync(X+soffset,dev_X+soffset,chunklen*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
      cudaMemcpyAsync(B+soffset,dev_B+soffset,chunklen*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
    }
    if (chunkrem!=0) {
      soffset=istream*chunklen;
      cudaMemcpyAsync(X+soffset,dev_X+soffset,chunkrem*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
      cudaMemcpyAsync(B+soffset,dev_B+soffset,chunkrem*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
    }
    for (istream=0; istream<=nstreams; istream++ ) 
      cudaStreamSynchronize(stream[istream]);
    cudaDeviceSetCacheConfig(cudaFuncCachePreferNone);
    for (istream=0; istream<=nstreams; istream++ ) 
      cudaStreamDestroy(stream[istream]);
    /*free allocated memory*/
    cudaFree(dev_X);
    cudaFree(dev_A);
    cudaFree(dev_B);
    cudaHostUnregister(A);
    cudaHostUnregister(X);
    cudaHostUnregister(B);
    cudaError_t err=cudaGetLastError();
    if (cudaSuccess!=err) 
      printf("CUDA runtime error: %s@",cudaGetErrorString(err));
  }
  {
    cudaDeviceSynchronize();
    /*declare variables*/
    double* dev_X;
    double* dev_A;
    double* dev_B;
    int nthreads=32;
    int nstreams=8;
    /*calculate device dimensions with the cuda calculator*/
    dim3 dimGrid, dimBlock;
    dimBlock.x=nthreads;
    dimGrid.x=14;
    /*create streams*/
    int istream, soffset;
    cudaStream_t stream[nstreams+1];
    for (istream=0; istream<=nstreams; istream++ ) 
      cudaStreamCreate(&stream[istream]);
    int chunklen=n/nstreams;
    int chunkrem=n%nstreams;
    /*allocate device memory*/
    cudaMalloc(&dev_A,N *(N +20 )*sizeof(double));
    cudaHostRegister(A,N *(N +20 )*sizeof(double),cudaHostRegisterPortable);
    cudaMalloc(&dev_X,N *(N +20 )*sizeof(double));
    cudaHostRegister(X,N *(N +20 )*sizeof(double),cudaHostRegisterPortable);
    cudaMalloc(&dev_B,N *(N +20 )*sizeof(double));
    cudaHostRegister(B,N *(N +20 )*sizeof(double),cudaHostRegisterPortable);
    cudaDeviceSetCacheConfig(cudaFuncCachePreferL1);
    /*copy data from host to device*/
    cudaEventRecord(tstart,0);
    for (istream=0; istream<nstreams; istream++ ) {
      soffset=istream*chunklen;
      cudaMemcpyAsync(dev_A+soffset,A+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
      cudaMemcpyAsync(dev_X+soffset,X+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
      cudaMemcpyAsync(dev_B+soffset,B+soffset,chunklen*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
    }
    if (chunkrem!=0) {
      soffset=istream*chunklen;
      cudaMemcpyAsync(dev_A+soffset,A+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
      cudaMemcpyAsync(dev_X+soffset,X+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
      cudaMemcpyAsync(dev_B+soffset,B+soffset,chunkrem*sizeof(double),cudaMemcpyHostToDevice,stream[istream]);
    }
    cudaEventRecord(tstop,0);
    cudaEventSynchronize(tstop);
    cudaEventElapsedTime(&orcu_transfer,tstart,tstop);
    cudaEventRecord(start,0);
    /*invoke device kernel*/
    int orcu_var89556=1;
    int blks4chunk=dimGrid.x/nstreams;
    if (dimGrid.x%nstreams!=0) 
      blks4chunk++ ;
    for (istream=0; istream<nstreams; istream++ ) {
      soffset=istream*chunklen;
      orcu_kernel89595<<<blks4chunk,dimBlock,0,stream[istream]>>>(chunklen,orcu_var89556,dev_X+soffset,dev_A+soffset,dev_B+soffset);
    }
    if (chunkrem!=0) {
      soffset=istream*chunklen;
      orcu_kernel89595<<<blks4chunk,dimBlock,0,stream[istream]>>>(chunkrem,orcu_var89556,dev_X+soffset,dev_A+soffset,dev_B+soffset);
    }
    cudaEventRecord(stop,0);
    cudaEventSynchronize(stop);
    cudaEventElapsedTime(&orcu_elapsed,start,stop);
    /*copy data from device to host*/
    for (istream=0; istream<nstreams; istream++ ) {
      soffset=istream*chunklen;
      cudaMemcpyAsync(X+soffset,dev_X+soffset,chunklen*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
      cudaMemcpyAsync(B+soffset,dev_B+soffset,chunklen*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
    }
    if (chunkrem!=0) {
      soffset=istream*chunklen;
      cudaMemcpyAsync(X+soffset,dev_X+soffset,chunkrem*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
      cudaMemcpyAsync(B+soffset,dev_B+soffset,chunkrem*sizeof(double),cudaMemcpyDeviceToHost,stream[istream]);
    }
    for (istream=0; istream<=nstreams; istream++ ) 
      cudaStreamSynchronize(stream[istream]);
    cudaDeviceSetCacheConfig(cudaFuncCachePreferNone);
    for (istream=0; istream<=nstreams; istream++ ) 
      cudaStreamDestroy(stream[istream]);
    /*free allocated memory*/
    cudaFree(dev_X);
    cudaFree(dev_A);
    cudaFree(dev_B);
    cudaHostUnregister(A);
    cudaHostUnregister(X);
    cudaHostUnregister(B);
    cudaError_t err=cudaGetLastError();
    if (cudaSuccess!=err) 
      printf("CUDA runtime error: %s@",cudaGetErrorString(err));
  }
}
/*@ end @*/
/*@ end @*/
}

